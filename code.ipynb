{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer\n",
    ")\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "num_labels = 36\n",
    "tokenizer_name = 'xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('model', num_labels=num_labels).to(device)\n",
    "config = model.config\n",
    "max_model_length = config.max_position_embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_logit(logit):\n",
    "    res = []\n",
    "    for i in range(0, 36, 6):\n",
    "        x = logit[i:i+6]\n",
    "        res.append(torch.argmax(x))\n",
    "    res = torch.stack(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 0, 0, 0, 0]\n",
      "{'review': 'không gian hiện đại phục vu chu đáo', 'results': {'giai_tri': 0, 'luu_tru': 5, 'nha_hang': 0, 'an_uong': 0, 'di_chuyen': 0, 'mua_sam': 0}}\n"
     ]
    }
   ],
   "source": [
    "review_sentence = 'không gian hiện đại phục vu chu đáo'\n",
    "input = tokenizer(review_sentence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=64).to(device)\n",
    "logit = model(**input)[0][0]\n",
    "predict_results = convert_logit(logit).tolist()\n",
    "print(predict_results)\n",
    "RATING_ASPECTS = [\"giai_tri\", \"luu_tru\", \"nha_hang\", \"an_uong\", \"di_chuyen\", \"mua_sam\"]\n",
    "output = {\n",
    "        \"review\": review_sentence,\n",
    "        \"results\": {}\n",
    "      }\n",
    "for count, r in enumerate(RATING_ASPECTS):\n",
    "    output[\"results\"][r] = predict_results[count]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "import sys\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/21 16:27:43 WARN Utils: Your hostname, ThinkPad-W540 resolves to a loopback address: 127.0.1.1; using 192.168.48.120 instead (on interface wlp3s0)\n",
      "22/12/21 16:27:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/21 16:27:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caohainam/anaconda3/envs/add-std/lib/python3.8/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"ReviewStreamApp\")\n",
    "# create spark instance with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# creat the Streaming Context from the above spark context with window size 2 seconds\n",
    "ssc = StreamingContext(sc, 10)\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_ReviewApp\")\n",
    "# # read data from port 9009\n",
    "# dataStream = ssc.socketTextStream(\"localhost\",9009)\n",
    "# ssc = StreamingContext(sc, 10)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 1606)\n",
    "\n",
    "lines = socket_stream.window(20)\n",
    "lines.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(dataStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataStream.count().pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "# Only works for Jupyter Notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# while count < 10:\n",
    "    \n",
    "#     time.sleep( 3 )\n",
    "#     top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "#     top_10_df = top_10_tweets.toPandas()\n",
    "#     display.clear_output(wait=True)\n",
    "#     plt.figure( figsize = ( 10, 8 ) )\n",
    "#     sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "#     plt.show()\n",
    "#     count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from config import *\n",
    "import re\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATING_ASPECTS = [\"giai_tri\", \"luu_tru\", \"nha_hang\", \"an_uong\", \"di_chuyen\", \"mua_sam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    if score >= 4:\n",
    "        return 'POSITIVE'\n",
    "    elif score == 3:\n",
    "        return 'NORMAL'\n",
    "    else:\n",
    "        return 'NEGATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file...\n",
      "\n",
      "[['giai_tri', 'POSITIVE']]\n"
     ]
    }
   ],
   "source": [
    "print('\\nReading file...\\n')\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "for index, row in data.iterrows():\n",
    "    review = row['review']\n",
    "    input = tokenizer(review, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=64).to(device)\n",
    "    logit = model(**input)[0][0]\n",
    "    predict_results = utils.convert_logit(logit).tolist()\n",
    "    lines = []\n",
    "    \n",
    "    for count, r in enumerate(RATING_ASPECTS):\n",
    "        if predict_results[count] > 0:\n",
    "            lines.append([r, utils.get_sentiment(predict_results[count])])\n",
    "    print(lines)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = ['giai_tri', 'luu_tru', 'nha_hang', 'an_uong', 'van_chuyen', 'mua_sam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sân mới được nâng cấp, sạch và mới, đặc biệt là mặt sân. Phần chỗ ngồi đa phần không có ghế, ngồi bệ xi măng.\n"
     ]
    }
   ],
   "source": [
    "for index, row in data.iterrows():\n",
    "    review = row['review']\n",
    "    print(review)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/22 09:35:46 WARN Utils: Your hostname, ThinkPad-W540 resolves to a loopback address: 127.0.1.1; using 192.168.48.120 instead (on interface wlp3s0)\n",
      "22/12/22 09:35:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/22 09:35:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"ReviewStreamApp\")\n",
    "# create spark instance with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# creat the Streaming Context from the above spark context with window size 2 seconds\n",
    "ssc = StreamingContext(sc, 2)\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_ReviewApp\")\n",
    "# read data from port 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\",1606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    total_sum = total_sum if total_sum else (0,0,0,0)\n",
    "    pos = [field[0] for field in new_values]\n",
    "    neu = [field[1] for field in new_values]\n",
    "    neg = [field[2] for field in new_values]\n",
    "    count = [field[3] for field in new_values]\n",
    "\n",
    "    return sum(pos)+total_sum[0], sum(neu)+total_sum[1], sum(neg)+total_sum[2], sum(count)+total_sum[3]\n",
    "\n",
    "def print_data(df):\n",
    "    # extract the hashtags from dataframe and convert them into array\n",
    "    aspects = [str(t.aspect) for t in df.select(\"aspect\").collect()]\n",
    "    # extract the counts from dataframe and convert them into array\n",
    "    pos = [p.pos for p in df.select(\"pos\").collect()]\n",
    "    neu = [p.neu for p in df.select(\"neu\").collect()]\n",
    "    neg = [p.neg for p in df.select(\"neg\").collect()]\n",
    "    request_data = {'label': str(aspects), 'data_pos': str(pos), 'data_neu': str(neu), 'data_neg': str(neg)}\n",
    "    print(request_data)\n",
    "\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        # Get spark sql singleton context from the current context\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        # convert the RDD to Row RDD\n",
    "        # row_rdd = rdd.map(lambda w: Row(aspect=w[0].encode(\"utf-8\"), pos=w[1][0], neu=w[1][1], neg=w[1][2],  aspect_count=w[1][3]))\n",
    "        row_rdd = rdd.map(lambda w: Row(aspect=w[0], pos=w[1][0], neu=w[1][1], neg=w[1][2],  aspect_count=w[1][3]))\n",
    "        # create a DF from the Row RDD\n",
    "        aspects_df = sql_context.createDataFrame(row_rdd)\n",
    "        # Register the dataframe as table\n",
    "        aspects_df.registerTempTable(\"aspects\")\n",
    "        # get the top 10 hashtags from the table using SQL and print them\n",
    "        aspect_counts_df = sql_context.sql(\"select aspect, aspect_count, pos, neu, neg from aspects order by aspect_count desc\")\n",
    "        aspect_counts_df.show()\n",
    "        # call this method to prepare top 10 hashtags DF and send them\n",
    "        # send_df_to_dashboard(aspect_counts_df)\n",
    "        print_data(aspect_counts_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(line):\n",
    "    # return line\n",
    "    data = line.split(\"||||\")\n",
    "    aspect = data[0]\n",
    "    if data[1] == \"POSITIVE\":\n",
    "        result = (aspect, 1, 0, 0)\n",
    "    elif data[1] == \"NORMAL\":\n",
    "        result = (aspect, 0, 1, 0)\n",
    "    else:\n",
    "        result = (aspect, 0, 0, 1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = dataStream.flatMap(split_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words.pprint()\n",
    "# dataStream.pprint()\n",
    "# words.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_data = dataStream.map(lambda x: split_word(x))\n",
    "stream_data = stream_data.map(lambda x: (x[0], (x[1], x[2], x[3], 1)))\n",
    "# adding the count of each hashtag to its last count\n",
    "stream_data = stream_data.updateStateByKey(aggregate_tags_count)\n",
    "# do processing for each RDD generated in each interval\n",
    "stream_data.foreachRDD(process_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_data.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = utils.config_db_params()\n",
    "connection = mysql.connector.connect(**params)\n",
    "mycursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'host': 'localhost',\n",
       " 'database': 'tourism_review',\n",
       " 'user': 'namch',\n",
       " 'password': 'Namch@1234'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkTableExists(dbcon, tablename):\n",
    "    dbcur = dbcon.cursor()\n",
    "    dbcur.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_name = '{0}'\n",
    "        \"\"\".format(tablename.replace('\\'', '\\'\\'')))\n",
    "    if dbcur.fetchone()[0] == 1:\n",
    "        dbcur.close()\n",
    "        return True\n",
    "\n",
    "    dbcur.close()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkTableExists(connection, 'customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"CREATE TABLE review_1 (review VARCHAR(255), giai_tri INT, luu_tru INT, nha_hang INT, an_uong INT, di_chuyen INT, mua_sam INT, time VARCHAR(20))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"INSERT INTO review_1 (review, giai_tri, luu_tru, nha_hang, an_uong, di_chuyen, mua_sam, time) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "now = datetime.now()\n",
    "time = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "val = (\"John\", 1, 1, 1, 1, 1, 1, time)\n",
    "mycursor.execute(sql, val)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 12, 27, 22, 6, 58, 909713)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "def config(filename='database.ini', section='mysql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    "\n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'host': 'localhost',\n",
       " 'database': 'tourism_review',\n",
       " 'user': 'namch',\n",
       " 'password': 'Namch@1234'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, end_time = '2022-12-26', '2022-12-29'\n",
    "# query = f\"SELECT * FROM review where time > {start_time}\"\n",
    "# query = f'select * from review where giai_tri=5'\n",
    "query = f\"select * from review where time between {start_time} and {end_time}\"\n",
    "mycursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "add-std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92c7cc7c25c514ecee4a3bbe5bd84fbb3cc2c3ab4cc06360af535b28c962c4fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
